.. _basic-minimalist-example:

Minimalist Example
==================

Now comes a minimalist example featuring a tiny but complete lexical analyser.
It displays the necessary Quex input code and the corresponding C/C++ code.
Not much of a syntax is explained.  Nevertheless, the glimpse at the sources
might prepare the reader's mind for the upcoming detailed discussion, or even
provide enough insight to engineer a customized lexer. 

Let us start with a `tiny.qx` file as input to the lexical analyser generator:

.. code-block:: cpp

    header {
    #ifdef __cplusplus
    #  include <cstdlib>   // for: atoi()
    #else
    #  include <stdlib.h>  // for: atoi()
    #endif
    }

    mode ONE_AND_ONLY
    {
        <<EOF>>     => QUEX_TKN_TERMINATION;

        [ \t\r\n]+  {}
        "="         => QUEX_TKN_OP_EQUAL;
        [0-9]+      => QUEX_TKN_NUMBER(number=atoi((char*)Lexeme));
        [_a-zA-Z]+  => QUEX_TKN_IDENTIFIER(Lexeme);
    }

First, a C standard header is included in a `header` section to be pasted
inside the generated code's header. The included header declares the function
`atoi()` which is used in the code fragments below.  The keyword `mode`
signalizes the definition of a lexer mode. All pattern action pairs need to be
related to a mode. In the simple example there is only one mode `ONE_AND_ONLY`
that collects all patterns to be matched against. The pattern actions simply
'send' a token as a reaction to a matched pattern. Assume that the content
mentioned above is stored in a file called `tiny.qx`. To generate C++ code,
Quex can now be invoked with

.. code-block:: bash

   > quex -i tiny.qx -o tiny

Plain C code is generated by

.. code-block:: bash

   > quex -i tiny.qx -o tiny --language C

This will create some source code to be compiled later on. The following C++ 
program demonstrates the usage of the generated lexical analyzer engine:

.. code-block:: cpp

    #include <fstream>    
    #include <iostream> 

    #include "tiny"

    int main(int argc, char** argv) 
    {        
        quex::Token*  token_p = 0x0;
        quex::tiny    tlex("example.txt", /* Converter */NULL);

        do {
            tlex.receive(&token_p);  

            std::cout << token_p->type_id_name() << std::endl;

        } while(    token_p->id     != QUEX_TKN_TERMINATION 
                 && tlex.error_code == E_Error_NONE );

        return 0;
    }

The C equivalent is:

.. code-block:: cpp

    #include <stdio.h>
    #include "tiny.h"

    int main(int argc, char** argv)
    {
        quex_Token*  token_p = 0x0;
        quex_tiny    tlex;

        QUEX_NAME(from_file_name)(&tlex, "example.txt", /* Converter */NULL);

        do {
            QUEX_NAME(receive)(&tlex, &token_p);

            printf("%s\n", QUEX_NAME_TOKEN(map_id_to_name)(token_p->id));

        } while(    token_p->id     != QUEX_TKN_TERMINATION
                 && tlex.error_code == E_Error_None );

        return 0;
    }

This program implements some user application for the generated lexer. It
contains a loop to read tokens from that input stream, prints the token's type
and exits as soon as the terminating token id is received. Storing one of the
two fragments in in a file called `lexer.cpp` or `lexer.c`, the following
compiles the lexer application. For C++:

.. code-block:: bash

   > g++  lexer.cpp  tiny.cpp -I$QUEX_PATH -I. -o lexer 

For plain C:

.. code-block:: bash

   > gcc  lexer.c  tiny.c -I$QUEX_PATH -I. -o lexer 

The choice of the GNU compiler `g++` and respectively `gcc` is, of course, not
mandatory. Any reasonably Standard compliant compiler will do. After the above
command terminates, there is an application called 'lexer' in the present
directory that can be sicked on some input.  Assume that `example.txt` contains
the following content:

.. code-block:: c

    James_Bond = 007

Then, a call to `lexer` will produce the following output:

.. code-block:: bash

    IDENTIFIER
    OP_EQUAL
    NUMBER

This minimalist minimalist example is located in the ``demo/*/00-Minimalist``
subdirectory of the distribution (replace the ``*`` with the programming
language's name). As a first exercise, one might try to modify the example to
print the token members ``number`` and ``text`` along with the token
identifier's name.

If it the lexical analyzer needs to be distributed in completely in source
code, then the command line option ``--source-package`` comes handy[#f1]_. That
is,

.. code-block:: bash

   > quex -i tiny.qx -o tiny --source-package my-package

The source package and the generated lexical analyzer are then located in
directory ``my-package``. Now, for compilation the include path of the
source package has to be passed as ``-I`` option, that is for the C++
example

.. code-block:: bash

   > $CC  lexer.cpp tiny.cpp -Imy-package -o lexer 

Further Playing
###############

The distribution's ``demo`` subdirectory is full of examples to play with.
They demonstrate plain lexers, indentation based lexical analysis (i.e. the
'off-side rule' :cite:`hutton1990parsing`) [#f2]_, applying converters to
Unicode, inclusion of other files, lexical engines running on other encodings
directly, input from command line and sockets, and the combination of multiple
lexical analyzers. In each subdirectory of the ``demo``-s there is a Makefile,
so ``make`` will produce the application to play with. Eventually, playing is
one of the best ways to familiarize oneself with these features.

.. rubric:: Footnotes

.. [#f1] Alternatively to source packaging, the compiler's pre-process could be
    used to generate a macro-expanded, all-included source file.  The GNU Compilers
    supports this via the '-E' command line option. The command line

    .. code-block:: bash

       > cat lexer.cpp >> tiny.cpp
       > g++ tiny.cpp -I$QUEX_PATH -I. -E -o my-packaged-lexer.cpp

    The first line generates a single source code file by pasting one at the
    end of the other.  The second line uses the C-preprocessor to expand all
    macros and include all include files into one single source file that can
    now be compiled independently. This, however, might include also some
    standard library headers which under normal conditions are not required in
    an independent source package.

.. [#f2]_ The term 'off-side rule' has been introduced by the British 
          Computer Scientist Peter John Landing (1930-2009) :cite:`turner2012some`. 
          It is derived from the FIFA football 'Law 11' :cite:`giulianotti2012football` 
          where a player is in offside position if he is closer to the opponent's 
          goal than any other opponent (except the opponent's goal keeper).
